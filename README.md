# ğŸ›¡ï¸ Fraud Detection â€” Stacking-Based Machine Learning System

An **end-to-end fraud detection project** built on the **IEEE-CIS Fraud Detection dataset**, using
multiple powerful base models, advanced preprocessing, and a **meta-learner (stacking ensemble)**
to achieve strong performance on highly imbalanced transaction data.

This repository focuses on:
- **Realistic time-based splits**
- **Out-of-fold (OOF) predictions**
- **Optuna hyperparameter optimization**
- **SHAP-based explainability**
- **Ensemble learning for robust fraud detection**

---

## âœ¨ Project Highlights

- ğŸ”¹ Designed for **extreme class imbalance** (fraud < 1%)
- ğŸ”¹ Multiple strong **base models** (LightGBM, CatBoost, Isolation Forest, Autoencoder)
- ğŸ”¹ **Stacking / Meta-learning** using XGBoost
- ğŸ”¹ Careful **data leakage prevention** via real-time splits
- ğŸ”¹ Model explainability using **SHAP**
- ğŸ”¹ Notebook-based, experiment-driven workflow

---

## ğŸ§  High-Level Approach

Fraud detection is not just classification â€” it is **risk ranking under asymmetric cost**.

This project follows a **stacking ensemble strategy**:

1. **Preprocess raw transaction data**
2. **Train multiple base models** using out-of-fold predictions
3. **Use base model outputs as features**
4. **Train a meta-learner** to combine model strengths
5. **Explain predictions** using SHAP values

This approach improves generalization and captures different fraud patterns that a single model may miss.

---

ğŸ“Š Dataset
ğŸ“¦ IEEE-CIS Fraud Detection Dataset

Transaction-level tabular data

Anonymized numerical & categorical features

Strong temporal dependency

Severe class imbalance

âš ï¸ Dataset files are not included and must be placed under:
Dataset/IEEE_CIS/

âš™ï¸ Preprocessing Pipeline
ğŸ•’ 1. Real-Time Train / Validation Split

ğŸ““ 01_preprocessing_real_time_split.ipynb

Time-based splitting

Prevents future information leakage

Mimics real-world production behavior

ğŸ·ï¸ 2. CatBoost-Optimized Dataset

ğŸ““ 01b_preprocessing_catboost_dataset.ipynb

Preserves categorical features

Optimized for CatBoostâ€™s native handling

ğŸ¤– Base Models

All base models use:

ğŸ” Out-of-Fold (OOF) predictions

ğŸ¯ Optuna hyperparameter optimization

ğŸ” SHAP explainability

ğŸŒ² LightGBM

ğŸ““ 02_lightgbm_base_model_oof_optuna_shap.ipynb

Gradient boosting for tabular data

Strong baseline with fast training

ğŸŒ Isolation Forest

ğŸ““ 03_isolation_forest_base_model_oof_optuna_shap.ipynb

Unsupervised anomaly detection

Captures rare and abnormal behavior

ğŸ§  Autoencoder

ğŸ““ 04_autoencoder_base_model_oof_optuna_shap_v2.ipynb

Neural network for anomaly detection

Learns compressed representations of normal transactions

ğŸ± CatBoost

ğŸ““ 05_catboost_base_model_oof_optuna_shap_fast.ipynb

Native categorical feature handling

Minimal preprocessing with strong performance

ğŸ§© Meta Learner â€” Stacking Ensemble
ğŸš€ XGBoost Meta-Model

ğŸ““ 06_xgb_meta_learner_optuna_shap_with_cat.ipynb

Trained on OOF predictions from all base models

Learns optimal combination of model outputs

Optuna-tuned hyperparameters

SHAP used to explain ensemble decisions

ğŸ”¥ This stage delivers the largest performance improvement.

ğŸ“ˆ Evaluation Strategy

Accuracy is misleading for fraud detection.

Metrics used:

ğŸ“Š ROC-AUC

ğŸ“‰ PR-AUC

ğŸ¯ Recall (Fraud Capture Rate)

ğŸ“Œ Precision

ğŸ† Recall@Top-K

âš–ï¸ Threshold tuning based on business cost

ğŸ” Explainability (SHAP)

SHAP is applied to:

Base models

Meta learner

Provides:

ğŸŒ Global feature importance

ğŸ” Local transaction-level explanations

ğŸ§¾ Audit-ready decision tracing

â–¶ï¸ Recommended Execution Order

1ï¸âƒ£ 01_preprocessing_real_time_split.ipynb
2ï¸âƒ£ 01b_preprocessing_catboost_dataset.ipynb

3ï¸âƒ£ Base Models

02_lightgbm_base_model_oof_optuna_shap.ipynb

03_isolation_forest_base_model_oof_optuna_shap.ipynb

04_autoencoder_base_model_oof_optuna_shap_v2.ipynb

05_catboost_base_model_oof_optuna_shap_fast.ipynb

4ï¸âƒ£ Meta Learner

06_xgb_meta_learner_optuna_shap_with_cat.ipynb

ğŸ›‘ Data Leakage Prevention

Strict safeguards against:

Using future transactions

Cross-time contamination

Post-event feature leakage

âœ” All splits occur before modeling

ğŸ§° Tech Stack

Python

Pandas / NumPy

Scikit-learn

LightGBM

CatBoost

XGBoost

Optuna

SHAP

Jupyter Notebook

ğŸ—ºï¸ Future Enhancements

ğŸ”„ Modular pipeline refactor

ğŸ“ Probability calibration

ğŸ“¡ Feature & prediction drift detection

ğŸŒ Real-time inference API

ğŸ“„ Model cards for compliance

ğŸ“œ License

Licensed under the MIT License
See LICENSE for details.

ğŸ‘¤ Author

Akshat
Fraud Detection & Machine Learning
GitHub: https://github.com/Dhingraakshat

## ğŸ—‚ï¸ Repository Structure

```text
Fraud-detection/
â”‚
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ Dataset/
â”‚   â””â”€â”€ IEEE_CIS/
â”‚       â””â”€â”€ (IEEE-CIS Fraud Detection dataset files)
â”‚
â”œâ”€â”€ Preprocessing/
â”‚   â”œâ”€â”€ 01_preprocessing_real_time_split.ipynb
â”‚   â””â”€â”€ 01b_preprocessing_catboost_dataset.ipynb
â”‚
â”œâ”€â”€ Base_Models/
â”‚   â”œâ”€â”€ 02_lightgbm_base_model_oof_optuna_shap.ipynb
â”‚   â”œâ”€â”€ 03_isolation_forest_base_model_oof_optuna_shap.ipynb
â”‚   â”œâ”€â”€ 04_autoencoder_base_model_oof_optuna_shap_v2.ipynb
â”‚   â””â”€â”€ 05_catboost_base_model_oof_optuna_shap_fast.ipynb
â”‚
â””â”€â”€ Meta_Learner/
    â””â”€â”€ 06_xgb_meta_learner_optuna_shap_with_cat.ipynb
