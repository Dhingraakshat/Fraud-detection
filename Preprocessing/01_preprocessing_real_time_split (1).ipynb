{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15d3c585",
      "metadata": {
        "id": "15d3c585"
      },
      "source": [
        "# 01 — Preprocessing (Real Transactions, Time‑Aware Split)\n",
        "This notebook:\n",
        "- Loads IEEE-CIS train_transaction + train_identity\n",
        "- Cleans and engineers interpretable features\n",
        "- Creates a chronological Train/Test split (real transactions only)\n",
        "- Saves:\n",
        "  - X_train_lgb.csv / X_test_lgb.csv (raw engineered features for LightGBM)\n",
        "  - X_train_scaled.csv / X_test_scaled.csv (OHE + scaled features for unsupervised models)\n",
        "  - y_train.csv / y_test.csv\n",
        "  - train_keys.csv / test_keys.csv (row_id ↔ TransactionID/DT mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7078d8c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7078d8c7",
        "outputId": "70507333-d16e-4d12-de7c-b0e9f1657e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "datapath = /content/drive/MyDrive/RThesis/\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install minimal requirements (Colab-safe)\n",
        "!pip -q install -U scikit-learn==1.3.2 imbalanced-learn==0.12.3 pandas==2.2.2 numpy==1.26.4 joblib==1.4.2\n",
        "\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Detect Colab + mount Drive\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Adjust if needed\n",
        "datapath = '/content/drive/MyDrive/RThesis/' if IN_COLAB else './'\n",
        "os.makedirs(datapath, exist_ok=True)\n",
        "\n",
        "print(\"datapath =\", datapath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "355cedc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "355cedc9",
        "outputId": "b3ae3bb4-af7c-4603-8266-56e6dd0df900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged shape: (590540, 434)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Load + merge datasets (IEEE-CIS)\n",
        "trans_path = os.path.join(datapath, \"train_transaction (1).csv\")\n",
        "iden_path  = os.path.join(datapath, \"train_identity (1).csv\")\n",
        "\n",
        "assert os.path.exists(trans_path), f\"Missing file: {trans_path}\"\n",
        "assert os.path.exists(iden_path),  f\"Missing file: {iden_path}\"\n",
        "\n",
        "trans = pd.read_csv(trans_path)\n",
        "iden  = pd.read_csv(iden_path)\n",
        "\n",
        "data = trans.merge(iden, how=\"left\", on=\"TransactionID\")\n",
        "print(\"Merged shape:\", data.shape)\n",
        "\n",
        "# Basic sanity\n",
        "required_cols = [\"TransactionID\", \"TransactionDT\", \"isFraud\"]\n",
        "missing = [c for c in required_cols if c not in data.columns]\n",
        "assert not missing, f\"Missing required columns: {missing}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "52ff6dcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52ff6dcb",
        "outputId": "bd77833a-6f30-4374-94d5-e027164cf622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nulls after fill (sample):\n",
            "TransactionID    0\n",
            "V231             0\n",
            "V242             0\n",
            "V241             0\n",
            "V240             0\n",
            "V239             0\n",
            "V238             0\n",
            "V237             0\n",
            "V236             0\n",
            "V235             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Basic cleaning: fill missing values\n",
        "# Numeric -> median; Categorical -> \"missing\"\n",
        "num_cols = data.select_dtypes(include=[np.number]).columns\n",
        "data[num_cols] = data[num_cols].fillna(data[num_cols].median())\n",
        "\n",
        "cat_cols = data.select_dtypes(include=[\"object\"]).columns\n",
        "data[cat_cols] = data[cat_cols].fillna(\"missing\")\n",
        "\n",
        "print(\"Nulls after fill (sample):\")\n",
        "print(data.isna().sum().sort_values(ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b47424f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b47424f8",
        "outputId": "ce968ba4-636e-4c1d-8e07-aa28e4ebcade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after drop: (590540, 26)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Drop unused / high-missing columns (keep only what you need for your engineered feature set)\n",
        "cols_to_drop = (\n",
        "    [f\"V{i}\" for i in range(1, 340)]\n",
        "    + [f\"id_{i:02d}\" for i in range(1, 39) if f\"id_{i:02d}\" not in [\"id_31\", \"id_30\", \"id_33\", \"id_36\", \"id_02\"]]\n",
        "    + [f\"D{i}\" for i in range(5, 16)]\n",
        "    + [f\"M{i}\" for i in range(1, 10)]\n",
        "    + [f\"C{i}\" for i in range(1, 15)]\n",
        "    + [\"P_emaildomain\", \"R_emaildomain\"]\n",
        ")\n",
        "\n",
        "data = data.drop(columns=cols_to_drop, errors=\"ignore\")\n",
        "print(\"Shape after drop:\", data.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ed1cc5",
      "metadata": {
        "id": "a1ed1cc5"
      },
      "source": [
        "## Feature engineering (human interpretable)\n",
        "Reuses your existing `add_features()` logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2bb12bd0",
      "metadata": {
        "id": "2bb12bd0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "def rolling_count_seconds(df: pd.DataFrame, entity_col: str, time_col: str, window_sec: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    For each row, count number of rows with the same entity within [time - window_sec, time].\n",
        "    Returns an array aligned to df.index. Sorts within each group; overall DF row order is preserved.\n",
        "    \"\"\"\n",
        "    counts = np.zeros(len(df), dtype=int)\n",
        "    for _, group in df.groupby(entity_col):\n",
        "        g = group.sort_values(time_col)\n",
        "        times = g[time_col].to_numpy()\n",
        "        idx = g.index.to_numpy()\n",
        "        left = 0\n",
        "        for right in range(len(times)):\n",
        "            while times[right] - times[left] > window_sec:\n",
        "                left += 1\n",
        "            counts[idx[right]] = right - left + 1\n",
        "    return counts\n",
        "\n",
        "def add_features(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = data.copy()\n",
        "\n",
        "    # Time features\n",
        "    if \"TransactionDT\" in out.columns:\n",
        "        day = np.floor(out[\"TransactionDT\"] / (24*60*60)).astype(\"int64\")\n",
        "        hour = np.floor((out[\"TransactionDT\"] % (24*60*60)) / 3600).astype(\"int64\")\n",
        "        out[\"Transaction_day\"] = day\n",
        "        out[\"Transaction_hour\"] = hour\n",
        "        out[\"is_night_txn\"] = out[\"Transaction_hour\"].isin(range(0, 7)).astype(\"int8\")\n",
        "\n",
        "        # Per-card/day transaction count\n",
        "        required = {\"card1\", \"Transaction_day\", \"TransactionID\"}\n",
        "        if required.issubset(out.columns):\n",
        "            grp = out.groupby([\"card1\", \"Transaction_day\"])[\"TransactionID\"].transform(\"count\")\n",
        "            out[\"trans_per_card_day\"] = grp.astype(\"int32\")\n",
        "        else:\n",
        "            out[\"trans_per_card_day\"] = 1\n",
        "\n",
        "    # Per-card amount statistics and aliases\n",
        "    if {\"card1\", \"TransactionAmt\"}.issubset(out.columns):\n",
        "        stats = (\n",
        "            out.groupby(\"card1\")[\"TransactionAmt\"]\n",
        "            .agg(avg_amt_per_card=\"mean\", amt_std_per_card=\"std\")\n",
        "            .reset_index()\n",
        "        )\n",
        "        out = out.merge(stats, on=\"card1\", how=\"left\")\n",
        "\n",
        "        if \"avg_amt_per_card\" in out.columns:\n",
        "            out[\"avg_amt_per_card_y\"] = out[\"avg_amt_per_card\"]\n",
        "        if \"amt_std_per_card\" in out.columns:\n",
        "            out[\"amt_std_per_card_y\"] = out[\"amt_std_per_card\"]\n",
        "\n",
        "        # Fill NaNs in std with its mean\n",
        "        if \"amt_std_per_card_y\" in out.columns:\n",
        "            std_mean = out[\"amt_std_per_card_y\"].mean()\n",
        "            out[\"amt_std_per_card_y\"] = out[\"amt_std_per_card_y\"].fillna(std_mean)\n",
        "\n",
        "        # Ratio and deviation features\n",
        "        if {\"TransactionAmt\", \"avg_amt_per_card_y\"}.issubset(out.columns):\n",
        "            out[\"freq_ratio_card_amt\"] = out[\"TransactionAmt\"] / (out[\"avg_amt_per_card_y\"] + 1e-5)\n",
        "        if {\"TransactionAmt\", \"avg_amt_per_card_y\", \"amt_std_per_card_y\"}.issubset(out.columns):\n",
        "            out[\"amt_deviation_card\"] = (\n",
        "                (out[\"TransactionAmt\"] - out[\"avg_amt_per_card_y\"]).abs()\n",
        "                / (out[\"amt_std_per_card_y\"] + 1e-5)\n",
        "            )\n",
        "\n",
        "    # Rolling counts per card over windows and time since last txn (no global sort)\n",
        "    if {\"card1\", \"TransactionDT\"}.issubset(out.columns):\n",
        "        out[\"trans_last_1h_card\"]  = rolling_count_seconds(out, \"card1\", \"TransactionDT\", 3600).astype(\"int32\")\n",
        "        out[\"trans_last_6h_card\"]  = rolling_count_seconds(out, \"card1\", \"TransactionDT\", 21600).astype(\"int32\")\n",
        "        out[\"trans_last_24h_card\"] = rolling_count_seconds(out, \"card1\", \"TransactionDT\", 86400).astype(\"int32\")\n",
        "        out[\"time_since_last_txn_card\"] = out.groupby(\"card1\")[\"TransactionDT\"].diff().fillna(0).astype(\"int64\")\n",
        "\n",
        "    # Unique devices per card in last 24h\n",
        "    if {\"DeviceInfo\", \"card1\", \"TransactionDT\"}.issubset(out.columns):\n",
        "        devices_per_card = np.zeros(len(out), dtype=int)\n",
        "        for _, group in out.groupby(\"card1\"):\n",
        "            g = group.sort_values(\"TransactionDT\")\n",
        "            times = g[\"TransactionDT\"].to_numpy()\n",
        "            devices = g[\"DeviceInfo\"].to_numpy()\n",
        "            idx = g.index.to_numpy()\n",
        "            left = 0\n",
        "            counter = Counter()\n",
        "            unique_count = 0\n",
        "            for right in range(len(times)):\n",
        "                d_r = devices[right]\n",
        "                if counter[d_r] == 0:\n",
        "                    unique_count += 1\n",
        "                counter[d_r] += 1\n",
        "                while times[right] - times[left] > 86400:\n",
        "                    d_l = devices[left]\n",
        "                    counter[d_l] -= 1\n",
        "                    if counter[d_l] == 0:\n",
        "                        unique_count -= 1\n",
        "                        del counter[d_l]\n",
        "                    left += 1\n",
        "                devices_per_card[idx[right]] = unique_count\n",
        "        out[\"devices_per_card_24h\"] = devices_per_card\n",
        "\n",
        "    # Unique cards per device in last 24h\n",
        "    if {\"DeviceInfo\", \"card1\", \"TransactionDT\"}.issubset(out.columns):\n",
        "        cards_per_device = np.zeros(len(out), dtype=int)\n",
        "        for _, group in out.groupby(\"DeviceInfo\"):\n",
        "            g = group.sort_values(\"TransactionDT\")\n",
        "            times = g[\"TransactionDT\"].to_numpy()\n",
        "            cards = g[\"card1\"].to_numpy()\n",
        "            idx = g.index.to_numpy()\n",
        "            left = 0\n",
        "            counter = Counter()\n",
        "            unique_count = 0\n",
        "            for right in range(len(times)):\n",
        "                c_r = cards[right]\n",
        "                if counter[c_r] == 0:\n",
        "                    unique_count += 1\n",
        "                counter[c_r] += 1\n",
        "                while times[right] - times[left] > 86400:\n",
        "                    c_l = cards[left]\n",
        "                    counter[c_l] -= 1\n",
        "                    if counter[c_l] == 0:\n",
        "                        unique_count -= 1\n",
        "                        del counter[c_l]\n",
        "                    left += 1\n",
        "                cards_per_device[idx[right]] = unique_count\n",
        "        out[\"cards_per_device_24h\"] = cards_per_device\n",
        "\n",
        "    # Change flags per card based on previous observation of the same card\n",
        "    # These use group-wise shift without changing global row order.\n",
        "    if {\"card1\", \"DeviceInfo\"}.issubset(out.columns):\n",
        "        out[\"device_change_flag\"] = (out.groupby(\"card1\")[\"DeviceInfo\"].shift() != out[\"DeviceInfo\"]).astype(\"int32\")\n",
        "    if {\"card1\", \"id_31\"}.issubset(out.columns):\n",
        "        out[\"browser_change_flag\"] = (out.groupby(\"card1\")[\"id_31\"].shift() != out[\"id_31\"]).astype(\"int32\")\n",
        "    if {\"card1\", \"id_30\"}.issubset(out.columns):\n",
        "        out[\"os_change_flag\"] = (out.groupby(\"card1\")[\"id_30\"].shift() != out[\"id_30\"]).astype(\"int32\")\n",
        "    if {\"card1\", \"id_33\"}.issubset(out.columns):\n",
        "        out[\"screen_resolution_change_flag\"] = (out.groupby(\"card1\")[\"id_33\"].shift() != out[\"id_33\"]).astype(\"int32\")\n",
        "    if {\"card1\", \"id_02\"}.issubset(out.columns):\n",
        "        out[\"geo_change_flag\"] = (out.groupby(\"card1\")[\"id_02\"].shift() != out[\"id_02\"]).astype(\"int32\")\n",
        "\n",
        "    # Unique counts by transform\n",
        "    if {\"card1\", \"id_36\"}.issubset(out.columns):\n",
        "        out[\"unique_ip_per_card\"] = out.groupby(\"card1\")[\"id_36\"].transform(\"nunique\")\n",
        "    if {\"card1\", \"id_36\"}.issubset(out.columns):\n",
        "        out[\"unique_cards_per_ip\"] = out.groupby(\"id_36\")[\"card1\"].transform(\"nunique\")\n",
        "\n",
        "    # Ratios using earlier features\n",
        "    if {\"trans_per_card_day\", \"devices_per_card_24h\"}.issubset(out.columns):\n",
        "        out[\"card_device_use_ratio\"] = out[\"trans_per_card_day\"] / (out[\"devices_per_card_24h\"] + 1)\n",
        "    if {\"card1\", \"time_since_last_txn_card\"}.issubset(out.columns):\n",
        "        mean_time = out.groupby(\"card1\")[\"time_since_last_txn_card\"].transform(\"mean\")\n",
        "        out[\"time_diff_ratio_to_card_mean\"] = out[\"time_since_last_txn_card\"] / (mean_time + 1e-5)\n",
        "\n",
        "    # Distance ratio\n",
        "    if {\"dist1\", \"dist2\"}.issubset(out.columns):\n",
        "        out[\"dist_ratio\"] = out[\"dist1\"] / (out[\"dist2\"].replace(0, np.nan) + 1e-5)\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8ad5e904",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ad5e904",
        "outputId": "f453aeab-5f25-4e13-c4e5-ec8ef338392a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After feature engineering: (590540, 52)\n",
            "New columns (sample): ['Transaction_day', 'Transaction_hour', 'is_night_txn', 'trans_per_card_day', 'avg_amt_per_card', 'amt_std_per_card', 'avg_amt_per_card_y', 'amt_std_per_card_y', 'freq_ratio_card_amt', 'amt_deviation_card', 'trans_last_1h_card', 'trans_last_6h_card', 'trans_last_24h_card', 'time_since_last_txn_card', 'devices_per_card_24h']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Sort chronologically (IMPORTANT for time-aware split + time_diff features)\n",
        "data = data.sort_values(\"TransactionDT\").reset_index(drop=True)\n",
        "\n",
        "# --- Apply feature engineering on the full chronological stream\n",
        "# This allows test rows to use historical context from train rows for rolling features.\n",
        "data_feat = add_features(data)\n",
        "\n",
        "print(\"After feature engineering:\", data_feat.shape)\n",
        "print(\"New columns (sample):\", [c for c in data_feat.columns if c not in data.columns][:15])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8e8bb8a5",
      "metadata": {
        "id": "8e8bb8a5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Build keys table for traceability (NOT used as features)\n",
        "keys = data_feat[[\"TransactionID\", \"TransactionDT\", \"isFraud\"]].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "07c8e9b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07c8e9b8",
        "outputId": "dc7729bd-14a7-4b14-ae86-a13c581384f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (590540, 38) y shape: (590540,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Drop raw columns that you *used* for feature engineering but do not want as model inputs\n",
        "cols_to_delete = [\n",
        "    \"TransactionID\",\n",
        "    \"addr1\", \"addr2\",\n",
        "    \"id_02\", \"id_30\", \"id_31\", \"id_33\", \"id_36\",\n",
        "    \"DeviceType\", \"DeviceInfo\",\n",
        "    \"avg_amt_per_card\", \"amt_std_per_card\",\n",
        "]\n",
        "\n",
        "def drop_columns_safe(df: pd.DataFrame, cols):\n",
        "    return df.drop(columns=[c for c in cols if c in df.columns], errors=\"ignore\")\n",
        "\n",
        "data_model = drop_columns_safe(data_feat, cols_to_delete)\n",
        "\n",
        "# Separate X/y, and remove raw TransactionDT from FEATURES (we keep derived time features instead)\n",
        "y = data_model[\"isFraud\"].astype(int)\n",
        "X = data_model.drop(columns=[\"isFraud\"], errors=\"ignore\").copy()\n",
        "\n",
        "if \"TransactionDT\" in X.columns:\n",
        "    X = X.drop(columns=[\"TransactionDT\"])  # avoid SMOTE/time artifacts\n",
        "\n",
        "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "851085fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "851085fb",
        "outputId": "ac62487e-695e-4250-e50d-803a89c822cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing feature cols after reindex (should be 0): 0\n",
            "Train fraud rate (overall): 0.03499000914417313\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Define the exact feature set to keep (matches your thesis feature list)\n",
        "categorical_cols = [\"ProductCD\", \"card4\", \"card6\"]\n",
        "\n",
        "numeric_cols = [\n",
        "    \"TransactionAmt\", \"card1\", \"card2\", \"card3\", \"card5\", \"dist1\", \"dist2\",\n",
        "    \"D1\", \"D2\", \"D3\", \"D4\",\n",
        "    \"Transaction_day\", \"Transaction_hour\", \"is_night_txn\",\n",
        "    \"trans_per_card_day\",\n",
        "    \"avg_amt_per_card_y\", \"amt_std_per_card_y\",\n",
        "    \"freq_ratio_card_amt\", \"amt_deviation_card\",\n",
        "    \"trans_last_1h_card\", \"trans_last_6h_card\", \"trans_last_24h_card\",\n",
        "    \"time_since_last_txn_card\",\n",
        "    \"devices_per_card_24h\", \"cards_per_device_24h\",\n",
        "    \"device_change_flag\", \"browser_change_flag\", \"os_change_flag\",\n",
        "    \"screen_resolution_change_flag\", \"geo_change_flag\",\n",
        "    \"unique_ip_per_card\", \"unique_cards_per_ip\",\n",
        "    \"card_device_use_ratio\", \"time_diff_ratio_to_card_mean\",\n",
        "    \"dist_ratio\",\n",
        "]\n",
        "\n",
        "feature_cols = categorical_cols + numeric_cols\n",
        "\n",
        "# Reindex to force consistent column order; fill missing with 0\n",
        "X = X.reindex(columns=feature_cols, fill_value=0)\n",
        "\n",
        "# Basic checks\n",
        "missing_feats = [c for c in feature_cols if c not in X.columns]\n",
        "print(\"Missing feature cols after reindex (should be 0):\", len(missing_feats))\n",
        "print(\"Train fraud rate (overall):\", y.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "04a90c46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04a90c46",
        "outputId": "3358304c-0cba-4b05-fa53-35a9140e5fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (472432, 38) Test: (118108, 38)\n",
            "Train fraud%: 3.5135215226741625 Test fraud%: 3.4409184813899145\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Chronological Train/Test split (real transactions only)\n",
        "# Use 80/20 by time. (Change TRAIN_FRAC if needed.)\n",
        "TRAIN_FRAC = 0.80\n",
        "n = len(X)\n",
        "split_idx = int(n * TRAIN_FRAC)\n",
        "\n",
        "X_train_lgb = X.iloc[:split_idx].reset_index(drop=True)\n",
        "X_test_lgb  = X.iloc[split_idx:].reset_index(drop=True)\n",
        "\n",
        "y_train = y.iloc[:split_idx].reset_index(drop=True)\n",
        "y_test  = y.iloc[split_idx:].reset_index(drop=True)\n",
        "\n",
        "train_keys = keys.iloc[:split_idx].reset_index(drop=True)\n",
        "test_keys  = keys.iloc[split_idx:].reset_index(drop=True)\n",
        "\n",
        "# Add row_id (per-split)\n",
        "train_keys.insert(0, \"row_id\", np.arange(len(train_keys)))\n",
        "test_keys.insert(0, \"row_id\", np.arange(len(test_keys)))\n",
        "\n",
        "print(\"Train:\", X_train_lgb.shape, \"Test:\", X_test_lgb.shape)\n",
        "print(\"Train fraud%:\", y_train.mean()*100, \"Test fraud%:\", y_test.mean()*100)\n",
        "\n",
        "# Ensure chronological split boundary\n",
        "assert train_keys[\"TransactionDT\"].max() <= test_keys[\"TransactionDT\"].min(),     \"Time split violated: train has later timestamps than test.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "be0744a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be0744a8",
        "outputId": "f27c9a54-b6bb-4f23-b4bb-74cab77362b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved:\n",
            " - /content/drive/MyDrive/RThesis/X_train_lgb.csv\n",
            " - /content/drive/MyDrive/RThesis/X_test_lgb.csv\n",
            " - /content/drive/MyDrive/RThesis/y_train.csv\n",
            " - /content/drive/MyDrive/RThesis/y_test.csv\n",
            " - /content/drive/MyDrive/RThesis/train_keys.csv\n",
            " - /content/drive/MyDrive/RThesis/test_keys.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Save LightGBM raw engineered features + labels + keys\n",
        "X_train_path = os.path.join(datapath, \"X_train_lgb.csv\")\n",
        "X_test_path  = os.path.join(datapath, \"X_test_lgb.csv\")\n",
        "y_train_path = os.path.join(datapath, \"y_train.csv\")\n",
        "y_test_path  = os.path.join(datapath, \"y_test.csv\")\n",
        "train_keys_path = os.path.join(datapath, \"train_keys.csv\")\n",
        "test_keys_path  = os.path.join(datapath, \"test_keys.csv\")\n",
        "\n",
        "X_train_lgb.to_csv(X_train_path, index=False)\n",
        "X_test_lgb.to_csv(X_test_path, index=False)\n",
        "\n",
        "y_train.to_frame(\"isFraud\").to_csv(y_train_path, index=False)\n",
        "y_test.to_frame(\"isFraud\").to_csv(y_test_path, index=False)\n",
        "\n",
        "train_keys.to_csv(train_keys_path, index=False)\n",
        "test_keys.to_csv(test_keys_path, index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" -\", X_train_path)\n",
        "print(\" -\", X_test_path)\n",
        "print(\" -\", y_train_path)\n",
        "print(\" -\", y_test_path)\n",
        "print(\" -\", train_keys_path)\n",
        "print(\" -\", test_keys_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f61fef47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f61fef47",
        "outputId": "ee078136-961e-41b1-96b6-885d47b93d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled shapes: (472432, 50) (118108, 50)\n",
            "Saved scaled data + pipeline:\n",
            " - /content/drive/MyDrive/RThesis/X_train_scaled.csv\n",
            " - /content/drive/MyDrive/RThesis/X_test_scaled.csv\n",
            " - /content/drive/MyDrive/RThesis/pipe_scaled.joblib\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Build scaled features for unsupervised models (OHE + StandardScaler)\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "def make_ohe():\n",
        "    # sklearn <1.2 uses sparse=..., >=1.2 uses sparse_output=...\n",
        "    try:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    except TypeError:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "ohe = make_ohe()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", ohe, categorical_cols),\n",
        "        (\"num\", \"passthrough\", numeric_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=True,\n",
        ")\n",
        "\n",
        "pipe_scaled = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"scale\", StandardScaler())\n",
        "])\n",
        "\n",
        "X_train_scaled = pipe_scaled.fit_transform(X_train_lgb)\n",
        "X_test_scaled  = pipe_scaled.transform(X_test_lgb)\n",
        "\n",
        "# Feature names\n",
        "feature_names = pipe_scaled.named_steps[\"preprocess\"].get_feature_names_out()\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
        "X_test_scaled_df  = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
        "\n",
        "print(\"Scaled shapes:\", X_train_scaled_df.shape, X_test_scaled_df.shape)\n",
        "\n",
        "# Save\n",
        "X_train_scaled_path = os.path.join(datapath, \"X_train_scaled.csv\")\n",
        "X_test_scaled_path  = os.path.join(datapath, \"X_test_scaled.csv\")\n",
        "pipe_path = os.path.join(datapath, \"pipe_scaled.joblib\")\n",
        "\n",
        "X_train_scaled_df.to_csv(X_train_scaled_path, index=False)\n",
        "X_test_scaled_df.to_csv(X_test_scaled_path, index=False)\n",
        "joblib.dump(pipe_scaled, pipe_path)\n",
        "\n",
        "print(\"Saved scaled data + pipeline:\")\n",
        "print(\" -\", X_train_scaled_path)\n",
        "print(\" -\", X_test_scaled_path)\n",
        "print(\" -\", pipe_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}