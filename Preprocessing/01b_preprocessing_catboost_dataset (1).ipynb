{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9b5d92cf",
      "metadata": {
        "id": "9b5d92cf"
      },
      "source": [
        "# 01b — Preprocessing for CatBoost (Human‑Interpretable + Categorical‑Friendly)\n",
        "\n",
        "This notebook is an **add-on** to your existing preprocessing.\n",
        "\n",
        "It:\n",
        "- Loads IEEE‑CIS `train_transaction` + `train_identity` and merges them\n",
        "- Reuses your **human‑interpretable feature engineering** (time, velocity, ratios, change flags)\n",
        "- Keeps **raw categorical fields** (email domains, device/browser/OS strings, address fields) so CatBoost can model them directly\n",
        "- Adds a few extra interpretable features: `log_amt`, `amt_decimal`, missingness indicators, and parsed device/email families\n",
        "- Creates the same **80/20 chronological split**\n",
        "- Saves:\n",
        "  - `X_train_cat.csv`, `X_test_cat.csv`\n",
        "  - `cat_feature_cols.json` (list of categorical columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4b9426de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b9426de",
        "outputId": "cac09b82-a52a-4f6e-c384-8860f1e5e2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "datapath = /content/drive/MyDrive/RThesis/\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- (Optional) Install deps (Colab-safe)\n",
        "try:\n",
        "    import pandas  # noqa\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                           \"pandas\", \"numpy\", \"scikit-learn\", \"imbalanced-learn\", \"joblib\"])\n",
        "\n",
        "import os, sys, json, random, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Detect Colab + mount Drive\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "# Adjust if needed\n",
        "datapath = \"/content/drive/MyDrive/RThesis/\" if IN_COLAB else \"./\"\n",
        "os.makedirs(datapath, exist_ok=True)\n",
        "print(\"datapath =\", datapath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "de5b1a17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de5b1a17",
        "outputId": "8f675e1c-5494-4950-ef75-6d2e0978a061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged shape: (590540, 434)\n",
            "Fraud rate (overall): 0.03499000914417313\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Load + merge IEEE-CIS (train)\n",
        "trans_path = os.path.join(datapath, \"train_transaction (1).csv\")\n",
        "iden_path  = os.path.join(datapath, \"train_identity (1).csv\")\n",
        "\n",
        "assert os.path.exists(trans_path), f\"Missing file: {trans_path}\"\n",
        "assert os.path.exists(iden_path),  f\"Missing file: {iden_path}\"\n",
        "\n",
        "trans = pd.read_csv(trans_path)\n",
        "iden  = pd.read_csv(iden_path)\n",
        "\n",
        "data = trans.merge(iden, how=\"left\", on=\"TransactionID\")\n",
        "print(\"Merged shape:\", data.shape)\n",
        "print(\"Fraud rate (overall):\", data[\"isFraud\"].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06db1f97",
      "metadata": {
        "id": "06db1f97"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Preserve missingness BEFORE filling (useful signal)\n",
        "miss_cols_for_count = [c for c in [\"DeviceInfo\", \"DeviceType\", \"P_emaildomain\", \"R_emaildomain\", \"addr1\", \"addr2\", \"id_30\", \"id_31\", \"id_33\"] if c in data.columns]\n",
        "if miss_cols_for_count:\n",
        "    data[\"missing_count_raw\"] = data[miss_cols_for_count].isna().sum(axis=1).astype(\"int16\")\n",
        "    data[\"missing_ratio_raw\"] = data[\"missing_count_raw\"] / max(1, len(miss_cols_for_count))\n",
        "else:\n",
        "    data[\"missing_count_raw\"] = 0\n",
        "    data[\"missing_ratio_raw\"] = 0.0\n",
        "\n",
        "# --- Basic cleaning: fill missing values\n",
        "# Numeric -> median; Categorical -> \"missing\"\n",
        "num_cols = data.select_dtypes(include=[np.number]).columns\n",
        "data[num_cols] = data[num_cols].fillna(data[num_cols].median())\n",
        "\n",
        "cat_cols_raw = data.select_dtypes(include=[\"object\"]).columns\n",
        "data[cat_cols_raw] = data[cat_cols_raw].fillna(\"missing\")\n",
        "\n",
        "print(\"Nulls after fill (top 10):\")\n",
        "print(data.isna().sum().sort_values(ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84959225",
      "metadata": {
        "id": "84959225"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Helper: causal rolling count within a lookback window (seconds)\n",
        "def rolling_count_seconds(df: pd.DataFrame, entity_col: str, time_col: str, window_sec: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    For each row, count number of rows with the same entity within [time - window_sec, time].\n",
        "    Returns an array aligned to df.index. Sorts within each group; overall DF row order is preserved.\n",
        "    \"\"\"\n",
        "    counts = np.zeros(len(df), dtype=int)\n",
        "    for _, group in df.groupby(entity_col):\n",
        "        g = group.sort_values(time_col)\n",
        "        times = g[time_col].to_numpy()\n",
        "        idx = g.index.to_numpy()\n",
        "        left = 0\n",
        "        for right in range(len(times)):\n",
        "            while times[right] - times[left] > window_sec:\n",
        "                left += 1\n",
        "            counts[idx[right]] = right - left + 1\n",
        "    return counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da7f2c0",
      "metadata": {
        "id": "1da7f2c0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Interpretable feature engineering (extends your 01 notebook)\n",
        "FREE_EMAIL_PROVIDERS = {\"gmail\", \"yahoo\", \"hotmail\", \"outlook\", \"icloud\", \"aol\", \"live\", \"msn\", \"protonmail\"}\n",
        "\n",
        "def _safe_split_left(x: str, sep: str = \" \") -> str:\n",
        "    if not isinstance(x, str) or x == \"missing\":\n",
        "        return \"missing\"\n",
        "    x = x.strip()\n",
        "    if not x:\n",
        "        return \"missing\"\n",
        "    return x.split(sep)[0].strip().lower()\n",
        "\n",
        "def _email_provider(domain: str) -> str:\n",
        "    if not isinstance(domain, str) or domain == \"missing\":\n",
        "        return \"missing\"\n",
        "    return domain.split(\".\")[0].lower() if \".\" in domain else domain.lower()\n",
        "\n",
        "def _email_tld(domain: str) -> str:\n",
        "    if not isinstance(domain, str) or domain == \"missing\":\n",
        "        return \"missing\"\n",
        "    parts = domain.split(\".\")\n",
        "    return parts[-1].lower() if len(parts) >= 2 else \"missing\"\n",
        "\n",
        "def add_features_catboost(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "\n",
        "    # Time features\n",
        "    if \"TransactionDT\" in out.columns:\n",
        "        day  = np.floor(out[\"TransactionDT\"] / (24*60*60)).astype(\"int64\")\n",
        "        hour = np.floor((out[\"TransactionDT\"] % (24*60*60)) / 3600).astype(\"int64\")\n",
        "        out[\"Transaction_day\"]  = day\n",
        "        out[\"Transaction_hour\"] = hour\n",
        "        out[\"is_night_txn\"]      = out[\"Transaction_hour\"].isin(range(0, 7)).astype(\"int8\")\n",
        "\n",
        "    # Amount transforms\n",
        "    if \"TransactionAmt\" in out.columns:\n",
        "        out[\"log_amt\"] = np.log1p(out[\"TransactionAmt\"])\n",
        "        out[\"amt_decimal\"] = (out[\"TransactionAmt\"] - np.floor(out[\"TransactionAmt\"])).round(6)\n",
        "        out[\"is_round_amt\"] = (out[\"amt_decimal\"] < 1e-6).astype(\"int8\")\n",
        "\n",
        "    # Missingness: computed BEFORE filling and stored as *_raw\n",
        "    if \"missing_count_raw\" in out.columns:\n",
        "        out[\"missing_count\"] = out[\"missing_count_raw\"].astype(\"int16\")\n",
        "        out[\"missing_ratio\"] = out[\"missing_ratio_raw\"].astype(float)\n",
        "    else:\n",
        "        out[\"missing_count\"] = 0\n",
        "        out[\"missing_ratio\"] = 0.0\n",
        "\n",
        "    # “Has identity” flag (interpretable)\n",
        "    if \"DeviceInfo\" in out.columns and \"DeviceType\" in out.columns:\n",
        "        out[\"has_identity\"] = ((out[\"DeviceInfo\"] != \"missing\") | (out[\"DeviceType\"] != \"missing\")).astype(\"int8\")\n",
        "    else:\n",
        "        out[\"has_identity\"] = 0\n",
        "\n",
        "    # Parsed categorical families\n",
        "    out[\"os_family\"] = out[\"id_30\"].apply(lambda x: _safe_split_left(x, \" \")) if \"id_30\" in out.columns else \"missing\"\n",
        "    out[\"browser_family\"] = out[\"id_31\"].apply(lambda x: _safe_split_left(x, \" \")) if \"id_31\" in out.columns else \"missing\"\n",
        "\n",
        "    if \"P_emaildomain\" in out.columns:\n",
        "        out[\"email_provider\"] = out[\"P_emaildomain\"].apply(_email_provider)\n",
        "        out[\"email_tld\"]      = out[\"P_emaildomain\"].apply(_email_tld)\n",
        "        out[\"is_free_email\"]  = out[\"email_provider\"].isin(FREE_EMAIL_PROVIDERS).astype(\"int8\")\n",
        "    else:\n",
        "        out[\"email_provider\"] = \"missing\"\n",
        "        out[\"email_tld\"]      = \"missing\"\n",
        "        out[\"is_free_email\"]  = 0\n",
        "\n",
        "    out[\"device_brand\"] = out[\"DeviceInfo\"].apply(lambda x: _safe_split_left(x, \" \")) if \"DeviceInfo\" in out.columns else \"missing\"\n",
        "\n",
        "    # Per-card/day transaction count\n",
        "    if {\"card1\", \"Transaction_day\", \"TransactionID\"}.issubset(out.columns):\n",
        "        out[\"trans_per_card_day\"] = out.groupby([\"card1\", \"Transaction_day\"])[\"TransactionID\"].transform(\"count\").astype(\"int32\")\n",
        "    else:\n",
        "        out[\"trans_per_card_day\"] = 1\n",
        "\n",
        "    # Per-card amount statistics + deviation\n",
        "    if {\"card1\", \"TransactionAmt\"}.issubset(out.columns):\n",
        "        stats = out.groupby(\"card1\")[\"TransactionAmt\"].agg(avg_amt_per_card=\"mean\", amt_std_per_card=\"std\").reset_index()\n",
        "        out = out.merge(stats, on=\"card1\", how=\"left\")\n",
        "        out[\"avg_amt_per_card_y\"] = out[\"avg_amt_per_card\"]\n",
        "        std_mean = float(out[\"amt_std_per_card\"].mean()) if \"amt_std_per_card\" in out.columns else 1.0\n",
        "        out[\"amt_std_per_card_y\"] = out[\"amt_std_per_card\"].fillna(std_mean)\n",
        "\n",
        "        out[\"freq_ratio_card_amt\"] = out[\"TransactionAmt\"] / (out[\"avg_amt_per_card_y\"] + 1e-5)\n",
        "        out[\"amt_deviation_card\"] = (out[\"TransactionAmt\"] - out[\"avg_amt_per_card_y\"]).abs() / (out[\"amt_std_per_card_y\"] + 1e-5)\n",
        "\n",
        "    # Rolling counts and time gap\n",
        "    if {\"card1\", \"TransactionDT\"}.issubset(out.columns):\n",
        "        out[\"trans_last_1h_card\"]  = rolling_count_seconds(out, \"card1\", \"TransactionDT\", 3600).astype(\"int32\")\n",
        "        out[\"trans_last_6h_card\"]  = rolling_count_seconds(out, \"card1\", \"TransactionDT\", 21600).astype(\"int32\")\n",
        "        out[\"trans_last_24h_card\"] = rolling_count_seconds(out, \"card1\", \"TransactionDT\", 86400).astype(\"int32\")\n",
        "        out[\"time_since_last_txn_card\"] = out.groupby(\"card1\")[\"TransactionDT\"].diff().fillna(0).astype(\"int64\")\n",
        "\n",
        "    # Device/card uniqueness windows (24h)\n",
        "    if {\"DeviceInfo\", \"card1\", \"TransactionDT\"}.issubset(out.columns):\n",
        "        devices_per_card = np.zeros(len(out), dtype=int)\n",
        "        for _, group in out.groupby(\"card1\"):\n",
        "            g = group.sort_values(\"TransactionDT\")\n",
        "            times = g[\"TransactionDT\"].to_numpy()\n",
        "            devices = g[\"DeviceInfo\"].to_numpy()\n",
        "            idx = g.index.to_numpy()\n",
        "            left = 0\n",
        "            counter = Counter()\n",
        "            unique_count = 0\n",
        "            for right in range(len(times)):\n",
        "                d_r = devices[right]\n",
        "                if counter[d_r] == 0:\n",
        "                    unique_count += 1\n",
        "                counter[d_r] += 1\n",
        "                while times[right] - times[left] > 86400:\n",
        "                    d_l = devices[left]\n",
        "                    counter[d_l] -= 1\n",
        "                    if counter[d_l] == 0:\n",
        "                        unique_count -= 1\n",
        "                        del counter[d_l]\n",
        "                    left += 1\n",
        "                devices_per_card[idx[right]] = unique_count\n",
        "        out[\"devices_per_card_24h\"] = devices_per_card\n",
        "\n",
        "        cards_per_device = np.zeros(len(out), dtype=int)\n",
        "        for _, group in out.groupby(\"DeviceInfo\"):\n",
        "            g = group.sort_values(\"TransactionDT\")\n",
        "            times = g[\"TransactionDT\"].to_numpy()\n",
        "            cards = g[\"card1\"].to_numpy()\n",
        "            idx = g.index.to_numpy()\n",
        "            left = 0\n",
        "            counter = Counter()\n",
        "            unique_count = 0\n",
        "            for right in range(len(times)):\n",
        "                c_r = cards[right]\n",
        "                if counter[c_r] == 0:\n",
        "                    unique_count += 1\n",
        "                counter[c_r] += 1\n",
        "                while times[right] - times[left] > 86400:\n",
        "                    c_l = cards[left]\n",
        "                    counter[c_l] -= 1\n",
        "                    if counter[c_l] == 0:\n",
        "                        unique_count -= 1\n",
        "                        del counter[c_l]\n",
        "                    left += 1\n",
        "                cards_per_device[idx[right]] = unique_count\n",
        "        out[\"cards_per_device_24h\"] = cards_per_device\n",
        "\n",
        "    # Change flags\n",
        "    if {\"card1\", \"DeviceInfo\"}.issubset(out.columns):\n",
        "        out[\"device_change_flag\"] = (out.groupby(\"card1\")[\"DeviceInfo\"].shift() != out[\"DeviceInfo\"]).astype(\"int32\")\n",
        "    if {\"card1\", \"id_31\"}.issubset(out.columns):\n",
        "        out[\"browser_change_flag\"] = (out.groupby(\"card1\")[\"id_31\"].shift() != out[\"id_31\"]).astype(\"int32\")\n",
        "    if {\"card1\", \"id_30\"}.issubset(out.columns):\n",
        "        out[\"os_change_flag\"] = (out.groupby(\"card1\")[\"id_30\"].shift() != out[\"id_30\"]).astype(\"int32\")\n",
        "    if {\"card1\", \"id_33\"}.issubset(out.columns):\n",
        "        out[\"screen_resolution_change_flag\"] = (out.groupby(\"card1\")[\"id_33\"].shift() != out[\"id_33\"]).astype(\"int32\")\n",
        "\n",
        "    # Ratios\n",
        "    if {\"trans_per_card_day\", \"devices_per_card_24h\"}.issubset(out.columns):\n",
        "        out[\"card_device_use_ratio\"] = out[\"trans_per_card_day\"] / (out[\"devices_per_card_24h\"] + 1)\n",
        "    if {\"card1\", \"time_since_last_txn_card\"}.issubset(out.columns):\n",
        "        mean_time = out.groupby(\"card1\")[\"time_since_last_txn_card\"].transform(\"mean\")\n",
        "        out[\"time_diff_ratio_to_card_mean\"] = out[\"time_since_last_txn_card\"] / (mean_time + 1e-5)\n",
        "    if {\"dist1\", \"dist2\"}.issubset(out.columns):\n",
        "        out[\"dist_ratio\"] = out[\"dist1\"] / (out[\"dist2\"].replace(0, np.nan) + 1e-5)\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4406db99",
      "metadata": {
        "id": "4406db99"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Sort chronologically + engineer features on full stream\n",
        "data = data.sort_values(\"TransactionDT\").reset_index(drop=True)\n",
        "data_feat = add_features_catboost(data)\n",
        "print(\"After feature engineering:\", data_feat.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cb15da3",
      "metadata": {
        "id": "3cb15da3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Build CatBoost feature matrix\n",
        "\n",
        "cat_cols = [\n",
        "    \"ProductCD\", \"card4\", \"card6\",\n",
        "    \"addr1\", \"addr2\",\n",
        "    \"P_emaildomain\", \"R_emaildomain\",\n",
        "    \"DeviceType\", \"DeviceInfo\",\n",
        "    \"id_30\", \"id_31\", \"id_33\",\n",
        "    \"os_family\", \"browser_family\", \"email_provider\", \"email_tld\", \"device_brand\",\n",
        "]\n",
        "\n",
        "num_cols = [\n",
        "    \"TransactionAmt\", \"log_amt\", \"amt_decimal\", \"is_round_amt\",\n",
        "    \"card1\", \"card2\", \"card3\", \"card5\",\n",
        "    \"dist1\", \"dist2\",\n",
        "    \"D1\", \"D2\", \"D3\", \"D4\",\n",
        "    \"Transaction_day\", \"Transaction_hour\", \"is_night_txn\",\n",
        "    \"trans_per_card_day\",\n",
        "    \"avg_amt_per_card_y\", \"amt_std_per_card_y\",\n",
        "    \"freq_ratio_card_amt\", \"amt_deviation_card\",\n",
        "    \"trans_last_1h_card\", \"trans_last_6h_card\", \"trans_last_24h_card\",\n",
        "    \"time_since_last_txn_card\",\n",
        "    \"devices_per_card_24h\", \"cards_per_device_24h\",\n",
        "    \"device_change_flag\", \"browser_change_flag\", \"os_change_flag\",\n",
        "    \"screen_resolution_change_flag\",\n",
        "    \"card_device_use_ratio\", \"time_diff_ratio_to_card_mean\",\n",
        "    \"dist_ratio\",\n",
        "    \"missing_count\", \"missing_ratio\", \"has_identity\", \"is_free_email\",\n",
        "]\n",
        "\n",
        "for c in cat_cols:\n",
        "    if c not in data_feat.columns:\n",
        "        data_feat[c] = \"missing\"\n",
        "for c in num_cols:\n",
        "    if c not in data_feat.columns:\n",
        "        data_feat[c] = 0\n",
        "\n",
        "feature_cols = cat_cols + num_cols\n",
        "\n",
        "y = data_feat[\"isFraud\"].astype(int).reset_index(drop=True)\n",
        "keys = data_feat[[\"TransactionID\", \"TransactionDT\"]].copy().reset_index(drop=True)\n",
        "\n",
        "X = data_feat[feature_cols].copy().reset_index(drop=True)\n",
        "\n",
        "# Force categorical to string\n",
        "for c in cat_cols:\n",
        "    X[c] = X[c].astype(str).fillna(\"missing\")\n",
        "\n",
        "print(\"X shape:\", X.shape, \"| y mean:\", y.mean())\n",
        "print(\"Categorical cols:\", len(cat_cols), \"| Numeric cols:\", len(num_cols))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c31fcb",
      "metadata": {
        "id": "83c31fcb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Chronological 80/20 split (same as 01)\n",
        "TRAIN_FRAC = 0.80\n",
        "n = len(X)\n",
        "split_idx = int(n * TRAIN_FRAC)\n",
        "\n",
        "X_train_cat = X.iloc[:split_idx].reset_index(drop=True)\n",
        "X_test_cat  = X.iloc[split_idx:].reset_index(drop=True)\n",
        "\n",
        "y_train = y.iloc[:split_idx].reset_index(drop=True)\n",
        "y_test  = y.iloc[split_idx:].reset_index(drop=True)\n",
        "\n",
        "train_keys = keys.iloc[:split_idx].reset_index(drop=True)\n",
        "test_keys  = keys.iloc[split_idx:].reset_index(drop=True)\n",
        "\n",
        "train_keys.insert(0, \"row_id\", np.arange(len(train_keys)))\n",
        "test_keys.insert(0, \"row_id\", np.arange(len(test_keys)))\n",
        "\n",
        "assert train_keys[\"TransactionDT\"].max() <= test_keys[\"TransactionDT\"].min(), \"Time split violated!\"\n",
        "print(\"Train:\", X_train_cat.shape, \"Test:\", X_test_cat.shape)\n",
        "print(\"Train fraud%:\", y_train.mean()*100, \"Test fraud%:\", y_test.mean()*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "421b8323",
      "metadata": {
        "id": "421b8323"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Save CatBoost-ready datasets\n",
        "X_train_path = os.path.join(datapath, \"X_train_cat.csv\")\n",
        "X_test_path  = os.path.join(datapath, \"X_test_cat.csv\")\n",
        "cat_cols_path = os.path.join(datapath, \"cat_feature_cols.json\")\n",
        "\n",
        "X_train_cat.to_csv(X_train_path, index=False)\n",
        "X_test_cat.to_csv(X_test_path, index=False)\n",
        "\n",
        "with open(cat_cols_path, \"w\") as f:\n",
        "    json.dump(cat_cols, f, indent=2)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" -\", X_train_path)\n",
        "print(\" -\", X_test_path)\n",
        "print(\" -\", cat_cols_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}